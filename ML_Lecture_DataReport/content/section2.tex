\pagebreak
\section{Phương pháp và nguồn thu thập dữ liệu}
\subsection{Nguồn dữ liệu}
Tập dữ liệu của nhóm được thu thập thông qua việc cào hơn 20,000 bài báo từ trang báo Thanh Niên (\url{https://thanhnien.vn}). Thông tin mỗi bài báo bao gồm đường link, tiêu đề, ngày đăng, nội dung, nhãn chính và các nhãn phụ nếu có.\\
Các bài báo được cào phân bổ đều theo chủ đề chính được gắn cho bài báo đó, gồm 7 tag chính: Thể thao, Thời sự, Chính trị, Kinh tế, Giáo dục, Sức khoẻ, Thế giới.
\subsection{Phương pháp thu thập dữ liệu}
Để thu thập được số lượng lớn bài báo mà vẫn đảm bảo phân loại chính xác chủ đề, nhóm cần giải quyết được 3 vấn đề chính:
\begin{itemize}
    \item Khả năng mở rộng nguồn dữ liệu, tức là tự động hoá việc lấy thêm đường link các bài báo.
    \item Phân tích cấu trúc HTML của một bài báo và trích xuất thông tin như tag, tiêu đề, nội dung,...
    \item Tổ chức lưu trữ dữ liệu đã thu thập được sao cho dễ truy xuất và mở rộng.
\end{itemize}
\subsubsection{Tìm kiếm khả năng mở rộng nguồn dữ liệu}
Hầu hết các trang báo mạng sẽ có các chuyên mục để người dùng có thể đọc thể loại mình muốn. Bằng việc giới hạn phạm vi thu thập trong các chuyên mục định sẵn, nhóm sẽ giải quyết được bài toán gán nhãn dữ liêu và giảm bớt công việc trong quá trình làm sạch.\\
Một vấn đề với cách tiếp cận này các cơ chế phân trang truyền thống trên VnExpress hay Dân Trí thường áp đặt giới hạn cứng ở mức 20-30 trang lịch sử, gây khó khăn cho việc xây dựng tập dữ liệu lớn.\\
Tuy nhiên, thông qua kỹ thuật phân tích gói tin HTTP bằng công cụ \verb|Burp Suite|, nhóm đã xác định được cơ chế truy xuất dữ liệu thông qua API ẩn của báo Thanh Niên. Khi nhấn vào nút "Xem Thêm", thay vì render lại toàn bộ trang web với nội dung mới, client sử dụng endpoint \verb|GET timelinelist/{id}/{page_number}.htm|
để trả về dữ liệu thô chứa đường link và hình ảnh của các bài báo trong một trang lịch sử và render nó ở cuối. Điểm đặc biệt nằm ở việc tham số \verb|page_number| có thể được tuỳ biến theo ý muốn, vượt qua con số 20 - 30, cho phép nhóm tự động hóa việc truy xuất các bài viết cũ nằm ngoài phạm vi hiển thị mặc định của giao diện người dùng.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{api.png}
    \caption{Kết quả trả về từ API ẩn của báo Thanh Niên}
    \label{fig:placeholder}
\end{figure}
\subsubsection{Trích xuất dữ liệu}
Nhóm sử dụng 3 thư viện Python cho việc trích xuất dữ liệu:
\begin{itemize}
    \item \verb|requests|: Thư viện dùng để gửi HTTP requests tới API cũng như đến các bài bào
    \item \verb|Beautiful Soup|: Thư viện dùng để trích xuất thông tin từ dạng HTML, XML,.. theo điều kiện mong muốn.
    \item \verb|sqlite3|: Connector tới sqlite, một hệ quản trị cơ sở dữ liệu SQL nhẹ, serverless, các database sẽ được xuất ra thành từng file, dùng cho việc lưu trữ các bài báo
\end{itemize}
Quy trình trích xuất dữ liệu từ một bài báo của nhóm gồm:
\begin{itemize}
    \item Lặp qua các id của từng chuyên mục và số lượng page theo cài đặt (150 trong lần chạy của nhóm).
    \item Với mỗi \verb|id| và \verb|page_number|, gửi một HTTP request tới API ẩn của báo Thanh Niên, dùng \verb|Beautiful Soup| lọc ra danh sách các đường link đến bài báo
    \item Gửi request tới đường link của từng bài báo đó nhận nội dung HTML trả về.
    \item Tiếp tục dùng \verb|Beautiful Soup| để duyệt qua các HTML tag chứa nội dung bài báo gồm tiêu đề, ngày đăng, nội dung, các tag phụ và trích xuất chúng. 
    \item Thực hiện lưu trữ nội dung đã trích xuất vào database.
\end{itemize}
\subsubsection{Tổ chức lưu trữ dữ liệu}
Nhóm đã sử dụng SQLite để tổ chức lưu trữ dữ liệu. Toàn bộ dữ liệu sau khi cào được nằm trong một file \verb|articles.sqlite3| với 2 bảng \verb|articles| và \verb|sub_tags|
\begin{lstlisting}[caption={Mã nguồn tạo bảng CSDL}]
CREATE TABLE articles (
    link TEXT PRIMARY KEY, 
    publication_date DATETIME,
    content TEXT,
    main_tag TEXT,
    source TEXT
);

CREATE TABLE sub_tags (
    link TEXT,
    tag TEXT,
    PRIMARY KEY (link, tag),
    FOREIGN KEY(link) REFERENCES articles(link)
);
\end{lstlisting}
Nhóm quyết định dùng mô hình cơ sở dữ liệu quan hệ (relational database) thay vì dùng file JSON hay CSV vì các lý do sau đây:
\begin{itemize}
    \item CSV không thích hợp để chứa nội dung các bài báo, vốn gồm nhiều dấu phẩy (,) và dấu nháy kép (").
    \item Việc cập nhật dữ liệu vào file JSON truyền thống gặp khó khăn lớn khi kích thước file tăng lên, do quy trình này thường đòi hỏi phải xử lý lại toàn bộ nội dung file để đảm bảo đúng cấu trúc cú pháp. 
    \item Với SQLite, nhóm có thể thiết lập quy trình ghi dữ liệu liên tục theo từng lô nhỏ (5 bài một lần insert) để đảm bảo hiệu suất mà có thể tránh các rủi ro kỹ thuật (mất mạng, sập nguồn) làm mất data trong quá trình cào bài báo.
    \item Việc dùng CSDL đã có ép kiểu (DATETIME) và đặt khoá chính từ trước sẽ giúp loại bỏ các dữ liệu trùng lặp hoặc gặp lỗi trong quá trình trích xuất, giúp làm giảm công việc cho khâu làm sạch.
\end{itemize}
\subsection{Quy trình làm sạch và tiền xử lý dữ liệu}
Trước khi đưa vào bất cứ mô hình nào, dữ liệu thô đều trải qua các bước làm sạch cơ bản để loại bỏ nhiễu:
\begin{itemize}
    \item \textbf{Xử lý giá trị thiếu và trùng lặp:} Thực hiện loại bỏ các mẫu dữ liệu bị khuyết thiếu thông tin quan trọng (như tiêu đề hoặc nội dung) và các bài viết bị trùng lặp hoàn toàn để tránh hiện tượng rò rỉ dữ liệu (\textit{data leakage}) giữa các tập huấn luyện và kiểm thử. Sau bước này, kích thước bộ dữ liệu còn lại là 20,777 bài viết.
    \item \textbf{Loại nhiễu định dạng:} Dữ liệu văn bản được quét để phát hiện và loại bỏ các đoạn mã HTML, CSS hoặc JavaScript không mang ngữ nghĩa, đảm bảo nội dung đầu vào hoàn toàn là văn bản tự nhiên.
    \item \textbf{Chuẩn hoá bảng mã Unicode:} Do đặc thù tiếng Việt có hai kiểu gõ phổ biến (dựng sẵn và tổ hợp), toàn bộ văn bản được chuẩn hoá về định dạng Unicode NFC để đảm bảo tính nhát quán trong biểu diễn ký tự.
\end{itemize}
Tuy nhiên, các mô hình có sự khác biệt đặc thù về cơ chế hoạt động, chủ yếu là sự khác biệt của Transformer so với phần còn lại. Vì thế giai đoạn tiền xử lý được chia thành hai luồng riêng biệt:
\begin{itemize}
    \item Đối với các mô hình cơ bản (TF-IDF + SVM, FastText), việc giảm chiều dữ liệu và thống nhất từ vựng là ưu tiên hàng đầu:
    \begin{itemize}
        \item \textbf{Chuẩn hoá ký tự thường (\textit{Lowercasing}):} Chuyển toàn bộ văn bản về chữ thường để giảm kích thước bộ từ điển mà không làm mất đi quá nhiều ngữ nghĩa quan trọng.
        \item \textbf{Tách từ (\textit{Word Segmentation}):} Sử dụng thư viện \texttt{pyvi} để gộp các từ ghép tiếng Việt, giúp nhận diện đúng đơn vị từ vựng.
        \item \textbf{Loại bỏ từ dừng (\textit{Stopword Removal}):} Loại bỏ các hư từ (\textit{stopwords}) dựa trên danh sách từ dừng tiếng Việt. Bước này giúp giảm nhiễu và tập trung vào các từ mang thông tin chính trị, kinh tế, xã hội đặc thù.
    \end{itemize}
    \item Đối với mô hình ngôn ngữ tiền huấn luyện PhoBERT, ngữ cảnh và cấu trúc câu đóng vai trò then chốt. Do đó, chiến lược xử lý được điều chỉnh như sau:
    \begin{itemize}
        \item \textbf{Giữ nguyên định dạng (\textit{No Lowercasing/Stopword Removal}):} Chúng tôi không chuyển văn bản về chữ thường và không loại bỏ từ dừng. Việc này nhằm bảo toàn cấu trúc ngữ pháp và thông tin về tên riêng, giúp cơ chế \textit{Attention} của PhoBERT hoạt động hiệu quả nhất.
        \item \textbf{Tách từ (\textit{Word Segmentation}):} Tương tự như luồng cơ bản, văn bản đầu vào cho PhoBERT cũng cần dược tách từ để đồng bộ với dữ liệu mà PhoBERT đã được huấn luyện trước đó.
    \end{itemize}
\end{itemize}
Cuối cùng, các nhãn đầu ra phải được mã hoá bằng cách ánh xạ các nhãn (gồm 7 nhãn cho ngữ cảnh của đồ án này) thành các con só nguyên (từ 0 tới 6 cho ngữ cảnh của đồ án này) để phục vụ quá trình tính toán của máy.
\subsection{Đảm bảo chất lượng dữ liệu}
Để đảm bảo mô hình phân loại đạt hiệu suất cao và có khả năng tổng quát hoá tốt, bộ dữ liệu huấn luyện cần thoả mãn các tiêu chí:
\begin{itemize}
    \item \textbf{Tính đầy đủ:} Các mẫu dữ liệu không được khuyết thiếu các trường thông tin quan trọng.
    \item \textbf{Tính sạch:} Dữ liệu văn bản không chứa các ký tự lạ, mã lỗi (HTML tags, JavaScript) hoặc các nội dung rác (quảng cáo hay link spam).
    \item \textbf{Tính nhất quán:} Dữ liệu phải đồng nhất về định dạng (Unicode NFC), ngôn ngữ (tiếng Việt) và cách gán nhãn chủ đề.
    \item \textbf{Tính cân bằng:} Số lượng mẫu dữ liệu giữa các nhãn không được chênh lệch quá lớn để tránh hiện tượng mô hình bị thiên vị (\textit{bias}) về phía lớp đa số.
\end{itemize}
Quy trình kiểm tra gồm 2 bước chính:
\begin{itemize}
    \item \textbf{Kiểm tra tự động (\textit{Automatic checks}):} Dựa trên các tiêu chí đã đề ra, xây dựng script để kiểm tra tự động quy trình EDA:
    \begin{itemize}
        \item \textbf{Kiểm tra giá trị rỗng:} Sử dụng \texttt{pandas} để quét toàn bộ DataFrame. Kết quả phát hiện 36 mẫu bị thiếu thông tin nghi là các quảng cáo, các mẫu đã được loại bỏ tự động.
        \item \textbf{Kiểm tra trùng lặp:} Rà soát dựa trên đường dẫn và nội dung bài báo để đảm bảo tính duy nhất.
        \item \textbf{Kiểm tra độ dài văn bản:} Tính toán độ dài ký tự của từng bài báo để phát hiện các bài quá ngắn hoặc quá dài.
        \item \textbf{Kiểm tra phân bố nhãn:} Thống kê số lượng bài viết theo từng chủ đề. Kết quả cho thấy dữ liệu đạt độ cân bằng rất tốt, không cần áp dụng các kỹ thuật tái lấy mẫu (\textit{Resmapling}).
    \end{itemize}
    \item \textbf{Kiểm tra thủ công (\textit{Manual checks}):} Lấy ngẫu nhiên 5 bài viết từ các nhãn khác nhau. Đọc lướt nội dung để xác minh nhãn chủ đề ban đầu có khớp với nội dung thực tế không. Kết quả là dữ liệu đạt độ sạch hoàn hảo.
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{img/null_checks.png}
  \captionsetup{width=0.6\textwidth}
  \caption{Đầu ra của bước kiểm tra giá trị rỗng. Cho thấy có 36 giá trị rỗng ở 2 cột \texttt{publication\_date} và \texttt{title}.}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{img/duplication_checks.png}
  \captionsetup{width=0.6\textwidth}
  \caption{Đầu ra của bước kiểm tra trùng lặp. Cho thấy không có giá trị trùng lặp giữa các mẫu.}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/length_analysis.png}
  \captionsetup{width=0.6\textwidth}
  \caption{Biểu đồ thể hiện độ dài của các bài báo. Các bài báo trong tập dữ liệu có độ dài chủ yếu phân bố từ 3000-4000 từ.}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/tag_distribution.png}
  \captionsetup{width=0.6\textwidth}
  \caption{Biểu đồ thể hiện phân bố nhãn của tập dữ liệu. Cho thấy sự phân bố đều một cách lý tưởng.}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/code_check.png}
  \captionsetup{width=0.6\textwidth}
  \caption{Biểu đồ thể hiện việc các bài báo có chứa mã thực thi hay không. Cho thấy không hề có mã thực thi trong dữ liệu.}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/random_checks.png}
  \captionsetup{width=0.6\textwidth}
  \caption{Đầu ra của bước kiểm tra thủ công sau cùng. Chọn ngẫu nhiên một bài báo rồi hiện ra khoảng 300 từ đầu tiên trong bài báo. Kết quả cho thấy dữ liệu sau khi phân tích khám phá đã sạch sẽ.}
\end{figure}
\subsection{Lưu trữ và quản lý dữ liệu}
Để đâm bảo khả năng tái lập (\textit{reproducibility}) và thuận tiện cho việc huấn luyện trên nhiều kiến trúc mô hình khác nhau, dữ liệu được quản lý theo quy trình 3 tầng:
\begin{itemize}
    \item \textbf{Dữ liệu trung gian:} Là dữ liệu sau bước phân tích khám phá, được lưu dưới dạng Pickle (\texttt{.pkl}) để giữ nguyên các thuộc tính đối tượng Python (như DataFrame) sau khi làm sạch sơ bộ, giúp tiết kiệm thời gian tải dữ liệu của quá trình kế tiếp.
    \item \textbf{Dữ liệu huấn luyện:} Được chia tách và lưu trữ dưới dạng CSV trong các thư mục riêng biệt, sẵn sàng nạp vào mô hình.
\end{itemize}
Tập dữ liệu sau cùng sẽ được phân tách thành 3 tập riêng biệt là Train, Validation và Test với tỉ lệ 80\%-10\%-10\%. Tuy nhiên cần áp dụng kỹ thuật lấy mẫu phân tầng (\textit{Stratified Sampling}) để đảm bảo phân bố đều các nhãn ra các tập dữ liệu. Dưới đây là bảng phân bố các nhãn:
\begin{table}[H]
\centering
\begin{tabular}{|c|l|l|l|}
    \hline
    \texttt{label\_encoded} & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
    \hline
    0 & 0.145960 & 0.145813 & 0.145813 \\
    \hline
    1 & 0.143854 & 0.143888 & 0.143888 \\
    \hline
    2 & 0.143794 & 0.143407 & 0.143888 \\
    \hline
    3 & 0.143132 & 0.143407 & 0.142926 \\
    \hline
    4 & 0.142410 & 0.142445 & 0.142445 \\
    \hline
    5 & 0.142109 & 0.141963 & 0.142445 \\
    \hline
    6 & 0.138740 & 0.139076 & 0.138595 \\
    \hline
\end{tabular}
\caption{Bảng so sánh tỉ lệ phân bố của 3 tập dữ liệu}
\label{label:my_label}
\end{table}
Dễ thấy từ bảng trên, tỉ lệ phân bố các nhãn của 3 tập dữ liệu khá đồng đều chứng tỏ quá trình phân chia dữ liệu đảm bảo được tính cân bằng.