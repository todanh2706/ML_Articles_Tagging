{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94c52f3",
   "metadata": {},
   "source": [
    "# **Preprocessing for BERT model**\n",
    "> [!IMPORTANT]\n",
    "> RUN THE ``basic_eda.ipynb`` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bff6f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20777 entries, 0 to 20812\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   link              20777 non-null  object\n",
      " 1   publication_date  20777 non-null  object\n",
      " 2   title             20777 non-null  object\n",
      " 3   content           20777 non-null  object\n",
      " 4   main_tag          20777 non-null  object\n",
      " 5   source            20777 non-null  object\n",
      " 6   content_length    20777 non-null  int64 \n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('df_for_bert.pkl')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944b174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Trưa 4.11, quyền Chủ tịch Liên đoàn Bóng đá Ma...\n",
      "1    Vụ việc FIFA bác đơn kháng cáo và giữ nguyên á...\n",
      "2    Nhà phê bình bóng đá Datuk Pekan Ramli đã bày ...\n",
      "3    Cựu tiền đạo Safee Sali - biểu tượng một thời ...\n",
      "4    Theo nhà báo Zulhelmi Zainal Azam của Astro Ar...\n",
      "Name: content_normalized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['content_normalized'] = df['content'].str.normalize('NFC')\n",
    "df['main_tag_normalized'] = df['main_tag'].str.normalize('NFC')\n",
    "\n",
    "print(df['content_normalized'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288ee8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "{'Thể thao': 0, 'Thế giới': 1, 'Giáo dục': 2, 'Kinh tế': 3, 'Chính trị': 4, 'Sức khỏe': 5, 'Thời sự': 6}\n",
      "\n",
      "After encoding:\n",
      "   main_tag  label_encoded\n",
      "0  Thể thao              0\n",
      "1  Thể thao              0\n",
      "2  Thể thao              0\n",
      "3  Thể thao              0\n",
      "4  Thể thao              0\n",
      "5  Thể thao              0\n",
      "6  Thể thao              0\n",
      "7  Thể thao              0\n",
      "8  Thể thao              0\n",
      "9  Thể thao              0\n",
      "\n",
      "Tags distribution check:\n",
      "label_encoded\n",
      "0    3032\n",
      "1    2989\n",
      "2    2987\n",
      "3    2974\n",
      "4    2959\n",
      "5    2953\n",
      "6    2883\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels_list = [\n",
    "    \"Thể thao\",\n",
    "    \"Thế giới\",\n",
    "    \"Giáo dục\",\n",
    "    \"Kinh tế\",\n",
    "    \"Chính trị\",\n",
    "    \"Sức khỏe\",\n",
    "    \"Thời sự\"\n",
    "]\n",
    "\n",
    "labels_mapping = {label: i for i, label in enumerate(labels_list)}\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "print(labels_mapping)\n",
    "\n",
    "df['label_encoded'] = df['main_tag_normalized'].map(labels_mapping)\n",
    "\n",
    "# Ensure all tags is appeared\n",
    "nan_count = df['label_encoded'].isnull().sum()\n",
    "\n",
    "if nan_count > 0:\n",
    "    print(f\"NaN: {nan_count}\")\n",
    "    all_labels_in_data = set(df['main_tag'].unique())\n",
    "    label_in_map = set(labels_mapping.keys())\n",
    "    unmapped_labels = all_labels_in_data - label_in_map\n",
    "\n",
    "print(\"\\nAfter encoding:\")\n",
    "print(df[['main_tag', 'label_encoded']].head(10))\n",
    "\n",
    "print(\"\\nTags distribution check:\")\n",
    "print(df['label_encoded'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8262a5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "KIỂM TRA KẾT QUẢ TÁCH TỪ (WORD SEGMENTATION)\n",
      "==================================================\n",
      "Kích thước tập dữ liệu: (20777, 11)\n",
      "Các cột hiện có: ['link', 'publication_date', 'title', 'content', 'main_tag', 'source', 'content_length', 'content_normalized', 'main_tag_normalized', 'label_encoded', 'content_segmented']\n",
      "\n",
      "--- Mẫu ngẫu nhiên tại index 20602 ---\n",
      "[Gốc (Normalized)]: \n",
      "AFP ngày 18.6 dẫn thông cáo của Nhà Trắng cho biết Tổng thống Mỹ Donald Trump sẽ gia hạn thêm 90 ngày để công ty mẹ ByteDance (Trung Quốc) thoái vốn khỏi ứng dụng chia sẻ video TikTok.\n",
      "\n",
      "\"Tổng thống Tr...\n",
      "--------------------\n",
      "[Đã tách từ (Segmented)]: \n",
      "AFP ngày 18.6 dẫn thông_cáo của Nhà_Trắng cho biết Tổng_thống Mỹ Donald_Trump sẽ gia_hạn thêm 90 ngày để công_ty_mẹ ByteDance ( Trung_Quốc ) thoái vốn khỏi ứng_dụng chia_sẻ video TikTok . \n",
      " \n",
      " \" Tổng_t...\n",
      "\n",
      "--- Kiểm tra sự xuất hiện của từ ghép (dấu '_') ---\n",
      "✓ Phát hiện từ ghép chuẩn 'bóng_đá' trong bài viết.\n",
      "✓ Phát hiện từ ghép chuẩn 'bóng_đá' trong bài viết.\n",
      "✓ Phát hiện từ ghép chuẩn 'bóng_đá' trong bài viết.\n",
      "=> Quy trình tách từ hoạt động ổn định.\n"
     ]
    }
   ],
   "source": [
    "# # Tokenization\n",
    "# from pyvi import ViTokenizer\n",
    "\n",
    "# df['content_segmented'] = df['content_normalized'].apply(lambda x: ViTokenizer.tokenize(str(x)))\n",
    "\n",
    "# X = df.drop(columns=['label_encoded', 'main_tag', 'main_tag_normalized', 'content'])\n",
    "\n",
    "import random\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"KIỂM TRA KẾT QUẢ TÁCH TỪ (WORD SEGMENTATION)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Kiểm tra cấu trúc dữ liệu\n",
    "print(f\"Kích thước tập dữ liệu: {df.shape}\")\n",
    "print(f\"Các cột hiện có: {df.columns.tolist()}\")\n",
    "\n",
    "# 2. So sánh trực quan Trước và Sau\n",
    "# Lấy ngẫu nhiên 1 dòng để soi\n",
    "random_idx = random.choice(df.index)\n",
    "sample_original = df['content_normalized'][random_idx]\n",
    "sample_segmented = df['content_segmented'][random_idx]\n",
    "\n",
    "print(f\"\\n--- Mẫu ngẫu nhiên tại index {random_idx} ---\")\n",
    "print(f\"[Gốc (Normalized)]: \\n{sample_original[:200]}...\") # In 200 ký tự đầu\n",
    "print(\"-\" * 20)\n",
    "print(f\"[Đã tách từ (Segmented)]: \\n{sample_segmented[:200]}...\")\n",
    "\n",
    "# 3. Kiểm tra nhanh các từ ghép đặc trưng\n",
    "# Nếu tách từ đúng, các từ này phải có dấu gạch dưới\n",
    "test_words = [\"thủ_tướng\", \"hôm_nay\", \"bóng_đá\", \"gia_đình\"]\n",
    "print(f\"\\n--- Kiểm tra sự xuất hiện của từ ghép (dấu '_') ---\")\n",
    "found_count = 0\n",
    "for text in df['content_segmented'].head(100): # Kiểm tra 100 bài đầu\n",
    "    for word in test_words:\n",
    "        if word in text:\n",
    "            print(f\"✓ Phát hiện từ ghép chuẩn '{word}' trong bài viết.\")\n",
    "            found_count += 1\n",
    "            break # Tìm thấy 1 từ là đủ để tin tưởng\n",
    "    if found_count > 2: break \n",
    "\n",
    "if found_count == 0:\n",
    "    print(\"⚠ CẢNH BÁO: Không tìm thấy các từ ghép phổ biến. Hãy kiểm tra lại thư viện pyvi.\")\n",
    "else:\n",
    "    print(\"=> Quy trình tách từ hoạt động ổn định.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42ce938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of starting samples: 20777\n",
      "------------------------------\n",
      "The number of samples in train: 16621 (80%)\n",
      "The number of samples in validation: 2078 (10%)\n",
      "The number of samples in test: 2078 (10%)\n",
      "\n",
      "Check the tag distribution in train:\n",
      "label_encoded\n",
      "0    0.145960\n",
      "1    0.143854\n",
      "2    0.143794\n",
      "3    0.143132\n",
      "4    0.142410\n",
      "5    0.142109\n",
      "6    0.138740\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Check the tag distribution in validation:\n",
      "label_encoded\n",
      "0    0.145813\n",
      "1    0.143888\n",
      "2    0.143407\n",
      "3    0.143407\n",
      "4    0.142445\n",
      "5    0.141963\n",
      "6    0.139076\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Check the tag distribution in validation:\n",
      "label_encoded\n",
      "0    0.145813\n",
      "1    0.143888\n",
      "2    0.143888\n",
      "3    0.142926\n",
      "4    0.142445\n",
      "5    0.142445\n",
      "6    0.138595\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['label_encoded', 'main_tag', 'main_tag_normalized'])\n",
    "y = df['label_encoded']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Sum of starting samples: {len(df)}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"The number of samples in train: {len(X_train)} ({len(X_train)/len(df):.0%})\")\n",
    "print(f\"The number of samples in validation: {len(X_val)} ({len(X_val)/len(df):.0%})\")\n",
    "print(f\"The number of samples in test: {len(X_test)} ({len(X_test)/len(df):.0%})\")\n",
    "\n",
    "print(\"\\nCheck the tag distribution in train:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nCheck the tag distribution in validation:\")\n",
    "print(y_val.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nCheck the tag distribution in validation:\")\n",
    "print(y_test.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6db9153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created final dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('train', exist_ok=True)\n",
    "os.makedirs('val', exist_ok=True)\n",
    "os.makedirs('test', exist_ok=True)\n",
    "\n",
    "X_train.to_csv('./train/X_train_bert.csv', index=False, encoding='utf-8')\n",
    "y_train.to_frame().to_csv('./train/y_train_bert.csv', index=False, encoding='utf-8')\n",
    "\n",
    "X_val.to_csv('./val/X_val_bert.csv', index=False, encoding='utf-8')\n",
    "y_val.to_frame().to_csv('./val/y_val_bert.csv', index=False, encoding='utf-8')\n",
    "\n",
    "X_test.to_csv('./test/X_test_bert.csv', index=False, encoding='utf-8')\n",
    "y_test.to_frame().to_csv('./test/y_test_bert.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\nCreated final dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
